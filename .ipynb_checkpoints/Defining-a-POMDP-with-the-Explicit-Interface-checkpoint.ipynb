{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a POMDP with the Explicit Interface\n",
    "\n",
    "In this tutorial we will define a version of the Tiger POMDP example [1] with the explicit interface of POMDPs.jl. To find out more about the explicit interface, please see this section of the POMDPs.jl documentation: [Explicit POMDP Interface](http://juliapomdp.github.io/POMDPs.jl/latest/explicit.html).\n",
    "\n",
    "[1] L. Pack Kaelbling, M. L. Littman, A. R. Cassandra, \"Planning and Action in Partially Observable Domain\", *Artificial Intelligence*, 1998."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using POMDPModelTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model type\n",
    "\n",
    "In the tiger POMDP, the agent is tasked with escaping from a room. There are two doors leading out of the room. Behind one of the doors is a tiger, and behind the other is sweet, sweet freedom. If the agent opens the door and finds the tiger, it gets eaten (and receives a reward of -100). If the agent opens the other door, it escapes and receives a reward of 10. The agent can also listen. Listening gives a noisy measurement of which door the tiger is hiding behind. Listening gives the agent the correct location of the tiger 85% of the time. The agent receives a reward of -1 for listening.\n",
    "\n",
    "The POMDP model of the problem is as follows:\n",
    "- The *state* is a Boolean value representing whether the tiger is on the left door (true) or on the right door (false)\n",
    "- The *action* is a Symbol indicating listening, opening the left door or opening the right door\n",
    "- The *observation* is a Boolean indicating whether the agent hears the tiger on the left (true) or on the right (false)\n",
    "- At each step there is a probability of hearing the tiger at the right location \n",
    "- Once the agent opens a door, it receives a reward and restarts at the initial state.\n",
    "\n",
    "The `TigerPOMDP` model type is a subtype of the `POMDP`. It is parameterized by the types used to represent the state, action, and observation respectively. The type also includes parameters that will be used in the reward and transitions definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerPOMDP"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct TigerPOMDP <: POMDP{Bool, Symbol, Bool} # POMDP{State, Action, Observation}\n",
    "    r_listen::Float64 # reward for listening (default -1)\n",
    "    r_findtiger::Float64 # reward for finding the tiger (default -100)\n",
    "    r_escapetiger::Float64 # reward for escaping (default 10)\n",
    "    p_listen_correctly::Float64 # prob of correctly listening (default 0.85)\n",
    "    discount_factor::Float64 # discount\n",
    "end\n",
    "\n",
    "TigerPOMDP() = TigerPOMDP(-1., -100., 10., 0.85, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States\n",
    "\n",
    "We define our state with a boolean that indicates weather or not the tiger is hiding behind the left door. If our state is true, the tiger is behind the left door. If its false, the tiger is behind the right door. \n",
    "\n",
    "We must implement the `states` function that returns the state space and `n_states` that returns the number of states. We should also define a `stateindex` function that returns the integer index of state `s`. \n",
    "For simple state types like `Int` or `Bool`, a default implementation of `stateindex` is provided as a convenience in [POMDPModelTools.jl](https://github.com/JuliaPOMDP/POMDPModelTools.jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.states(pomdp::TigerPOMDP) = [true, false]\n",
    "POMDPs.n_states(pomdp::TigerPOMDP) = 2\n",
    "POMDPs.stateindex(pomdp::TigerPOMDP, s::Bool) = s ? 1 : 2 ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "There are three actions in our problem. Once again, we represent the action space as an array of the actions in our problem. The actions function serve a similar purpose to the `states` function above. Since the action space is discrete, we can define the `actionindex` function that associates an integer index to each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.actions(pomdp::TigerPOMDP) = [:open_left, :open_right, :listen]\n",
    "POMDPs.n_actions(pomdp::TigerPOMDP) = 3\n",
    "function POMDPs.action_index(pomdp::TigerPOMDP, a::Symbol)\n",
    "    if a==:open_left\n",
    "        return 1\n",
    "    elseif a==:open_right\n",
    "        return 2\n",
    "    elseif a==:listen\n",
    "        return 3\n",
    "    end\n",
    "    error(\"invalid TigerPOMDP action: $a\")\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** if the actions available depends on the state, one must additionally implement the function `actions(pomdp, s)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State transitions\n",
    "\n",
    "Now that the states and actions are defined, the transition distribution can be specified. We can do so by implementing the `transition` function. It takes as input the pomdp model, a state and an action and returns a distribution over the next states. \n",
    "\n",
    "We first need a data type to represent the transition distribution. We must be able to sample a state from this object using `rand` and also query the probability mass of a certain state using `pdf`. One can create its own type if needed. POMDPModelTools provide useful [distribution types](https://juliapomdp.github.io/POMDPModelTools.jl/latest/distributions.html). \n",
    "\n",
    "In the tiger problem, since there are two state we can represent our distribution with one parameter corresponding to the probability of being in state `true` (tiger on the left). This can be done using the `BoolDistribution` type provided by POMDPModelTools. The transition model is described as follows: \n",
    "- The tiger always stays on the same side \n",
    "- Once we open the door, the problem resets, that is the tiger is spawned with equal probability behind one of the two doors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.transition(pomdp::TigerPOMDP, s::Bool, a::Symbol)\n",
    "    if a == :open_left || a == :open_right\n",
    "        # problem resets\n",
    "        return BoolDistribution(0.5) \n",
    "    elseif s\n",
    "        # tiger on the left stays on the left \n",
    "        return BoolDistribution(1.0)\n",
    "    else\n",
    "        return BoolDistribution(0.0)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "In the tiger problem there are two possible observations: hearing the tiger on the left or on the right. We represent them by a boolean. Similarly as for states and actions we must implement `observations`, `n_observations`, and `obsindex`.\n",
    "\n",
    "**Note:** `obsindex` for boolean observation is provided as a convenience function by POMDPModelTools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.observations(pomdp::TigerPOMDP) = [true, false]\n",
    "POMDPs.n_observations(pomdp::TigerPOMDP) = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation model captures the uncertainty in the agent's listening ability. When we listen, we receive a noisy measurement of the tiger's location. \n",
    "To implement the observation model with the explicit interface, one must implement the function `observation` which returns a distribution. Remember that observations are also represented by booleans, we can again use the `BoolDistribution` type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.observation(pomdp::TigerPOMDP, a::Symbol, s::Bool)\n",
    "    pc = pomdp.p_listen_correctly\n",
    "    if a == :listen \n",
    "        if s \n",
    "            return BoolDistribution(pc)\n",
    "        else\n",
    "            return BoolDistribution(1 - pc)\n",
    "        end\n",
    "    else\n",
    "        return BoolDistribution(0.5)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "The reward model caputres the immediate objectives of the agent. It recieves a large negative reward for opening the door with the tiger behind it (-100), gets a positive reward for opening the other door (+10), and a small penalty for listening (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward model\n",
    "function POMDPs.reward(pomdp::TigerPOMDP, s::Bool, a::Symbol)\n",
    "    r = 0.0\n",
    "    a == :listen ? (r+=pomdp.r_listen) : (nothing)\n",
    "    if a == :open_left\n",
    "        s ? (r += pomdp.r_findtiger) : (r += pomdp.r_escapetiger)\n",
    "    end\n",
    "    if a == :open_right\n",
    "        s ? (r += pomdp.r_escapetiger) : (r += pomdp.r_findtiger)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "POMDPs.reward(pomdp::TigerPOMDP, s::Bool, a::Symbol, sp::Bool) = reward(pomdp, s, a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Belief\n",
    "\n",
    "In POMDPs, we often represent our estimate of the current state with a belief, a distribution over states. Since we have two possible state, we can use the convenient `BoolDistribution` one more time. \n",
    "\n",
    "Implementing beliefs and their updaters can be tricky. Luckily, our solvers abstract away the belief updating. All you need to do is define a function that returns an initial distribution over states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.initialstate_distribution(pomdp::TigerPOMDP) = BoolDistribution(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about beliefs and belief updating you may look at [BeliefUpdaters.jl](https://github.com/JuliaPOMDP/BeliefUpdaters.jl) where a collection of belief representations and belief updaters. \n",
    "**Note:** It is also possible to create your own belief representation and update schemes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous Functions\n",
    "\n",
    "Let's define the `discount` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "POMDPs.discount(pomdp::TigerPOMDP) = pomdp.discount_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing the model in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now implemented all the functions necessary to solve or simulate the tiger pomdp with the explicit interface!\n",
    "\n",
    "To learn how to solve POMDPs offline: TODO[link to tutorial]\n",
    "\n",
    "To learn how to solve POMDPs online: TODO[link_to_tutorial]\n",
    "\n",
    "We can run a simulation of our model using the `stepthrough` function\n",
    "\n",
    "For more information on running simulations, see the simulation example [TODO: add link]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s = true\n",
      "a = :listen\n",
      "r = -1.0\n",
      "\n",
      "s = true\n",
      "a = :open_right\n",
      "r = 10.0\n",
      "\n",
      "s = false\n",
      "a = :open_left\n",
      "r = 10.0\n",
      "\n",
      "s = true\n",
      "a = :listen\n",
      "r = -1.0\n",
      "\n",
      "s = true\n",
      "a = :listen\n",
      "r = -1.0\n",
      "\n",
      "s = true\n",
      "a = :listen\n",
      "r = -1.0\n",
      "\n",
      "s = true\n",
      "a = :listen\n",
      "r = -1.0\n",
      "\n",
      "s = true\n",
      "a = :open_right\n",
      "r = 10.0\n",
      "\n",
      "s = false\n",
      "a = :listen\n",
      "r = -1.0\n",
      "\n",
      "s = false\n",
      "a = :listen\n",
      "r = -1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using POMDPSimulators\n",
    "using POMDPPolicies\n",
    "\n",
    "m = TigerPOMDP()\n",
    "\n",
    "# policy that takes a random action\n",
    "policy = RandomPolicy(m)\n",
    "\n",
    "for (s, a, r) in stepthrough(m, policy, \"s,a,r\", max_steps=10)\n",
    "    @show s\n",
    "    @show a\n",
    "    @show r\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
